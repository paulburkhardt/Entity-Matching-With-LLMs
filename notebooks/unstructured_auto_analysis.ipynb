{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langfuse.callback import CallbackHandler\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = '../datasets/error_analysis/sampled/auto_gpt3_gpt3_50_50_generations_sampled.csv'\n",
    "MODEL1 = \"gpt-3.5-turbo-0301\"\n",
    "MODEL2 = \"gpt-3.5-turbo-16k-0613\"\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATAPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "handler = CallbackHandler(os.environ.get(\"LF_PK\"), os.environ.get(\"LF_SK\"), os.environ.get(\"LF_HOST\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "task = \"\"\"Now explain your concisely how you made your decision and explicitly mention the attributes and values that had a high influence on your decision.\"\"\"\n",
    "task2 = \"\"\"Now explain concisely how you made your decision and explicitly mention the attributes and values that had a high influence on your decision.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "UNSTRUCTURED_ANALYSIS_PROMPT = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful AI.\"),\n",
    "            (\"human\", \"{user_prompt}\"),\n",
    "            (\"ai\", \"{ai_answer}\"),\n",
    "            (\"human\", task),\n",
    "        ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"unstructured_analysis\" not in df.columns:\n",
    "    df[\"unstructured_analysis\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>traceId</th>\n",
       "      <th>model</th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>eval</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>label</th>\n",
       "      <th>error</th>\n",
       "      <th>similarity</th>\n",
       "      <th>confidence</th>\n",
       "      <th>unstructured_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>136e98ba-ac14-42f4-8345-103a04fd95e3</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>2023-10-20T12:39:55.032Z</td>\n",
       "      <td>2023-10-20T12:40:03.460Z</td>\n",
       "      <td>Q: Are the following two products the same?\\nP...</td>\n",
       "      <td>First, we can compare the titles and descripti...</td>\n",
       "      <td>no match</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>FN</td>\n",
       "      <td>20%</td>\n",
       "      <td>90%</td>\n",
       "      <td>I made my decision by comparing the titles and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>136e98ba-ac14-42f4-8345-103a04fd95e3</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>2023-10-20T12:38:55.776Z</td>\n",
       "      <td>2023-10-20T12:39:08.718Z</td>\n",
       "      <td>Q: Are the following two products the same?\\nP...</td>\n",
       "      <td>First, we can compare the titles of the two pr...</td>\n",
       "      <td>The two products may match.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NONE</td>\n",
       "      <td>90%</td>\n",
       "      <td>80%</td>\n",
       "      <td>I made my decision based on the similarities b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>136e98ba-ac14-42f4-8345-103a04fd95e3</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>2023-10-20T12:38:41.714Z</td>\n",
       "      <td>2023-10-20T12:38:54.408Z</td>\n",
       "      <td>Q: Are the following two products the same?\\nP...</td>\n",
       "      <td>First, we can compare the titles of the two pr...</td>\n",
       "      <td>No match.</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>FN</td>\n",
       "      <td>60%</td>\n",
       "      <td>50%</td>\n",
       "      <td>I made my decision based on a comparison of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>136e98ba-ac14-42f4-8345-103a04fd95e3</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>2023-10-20T12:38:26.414Z</td>\n",
       "      <td>2023-10-20T12:38:39.759Z</td>\n",
       "      <td>Q: Are the following two products the same?\\nP...</td>\n",
       "      <td>First, we can compare the titles of the two pr...</td>\n",
       "      <td>no match</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>FN</td>\n",
       "      <td>40%</td>\n",
       "      <td>70%</td>\n",
       "      <td>I compared the titles of the two products and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>136e98ba-ac14-42f4-8345-103a04fd95e3</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>2023-10-20T12:38:15.109Z</td>\n",
       "      <td>2023-10-20T12:38:24.862Z</td>\n",
       "      <td>Q: Are the following two products the same?\\nP...</td>\n",
       "      <td>First, we can compare the titles and descripti...</td>\n",
       "      <td>no match</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>FN</td>\n",
       "      <td>40%</td>\n",
       "      <td>80%</td>\n",
       "      <td>I compared the titles and descriptions of the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                               traceId               model  \\\n",
       "0           0  136e98ba-ac14-42f4-8345-103a04fd95e3  gpt-3.5-turbo-0301   \n",
       "1           4  136e98ba-ac14-42f4-8345-103a04fd95e3  gpt-3.5-turbo-0301   \n",
       "2           5  136e98ba-ac14-42f4-8345-103a04fd95e3  gpt-3.5-turbo-0301   \n",
       "3           6  136e98ba-ac14-42f4-8345-103a04fd95e3  gpt-3.5-turbo-0301   \n",
       "4           7  136e98ba-ac14-42f4-8345-103a04fd95e3  gpt-3.5-turbo-0301   \n",
       "\n",
       "                  startTime                   endTime  \\\n",
       "0  2023-10-20T12:39:55.032Z  2023-10-20T12:40:03.460Z   \n",
       "1  2023-10-20T12:38:55.776Z  2023-10-20T12:39:08.718Z   \n",
       "2  2023-10-20T12:38:41.714Z  2023-10-20T12:38:54.408Z   \n",
       "3  2023-10-20T12:38:26.414Z  2023-10-20T12:38:39.759Z   \n",
       "4  2023-10-20T12:38:15.109Z  2023-10-20T12:38:24.862Z   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Q: Are the following two products the same?\\nP...   \n",
       "1  Q: Are the following two products the same?\\nP...   \n",
       "2  Q: Are the following two products the same?\\nP...   \n",
       "3  Q: Are the following two products the same?\\nP...   \n",
       "4  Q: Are the following two products the same?\\nP...   \n",
       "\n",
       "                                          completion  \\\n",
       "0  First, we can compare the titles and descripti...   \n",
       "1  First, we can compare the titles of the two pr...   \n",
       "2  First, we can compare the titles of the two pr...   \n",
       "3  First, we can compare the titles of the two pr...   \n",
       "4  First, we can compare the titles and descripti...   \n",
       "\n",
       "                          eval  predicted_label  label error similarity  \\\n",
       "0                     no match            False   True    FN        20%   \n",
       "1  The two products may match.             True   True  NONE        90%   \n",
       "2                    No match.            False   True    FN        60%   \n",
       "3                     no match            False   True    FN        40%   \n",
       "4                     no match            False   True    FN        40%   \n",
       "\n",
       "  confidence                              unstructured_analysis  \n",
       "0        90%  I made my decision by comparing the titles and...  \n",
       "1        80%  I made my decision based on the similarities b...  \n",
       "2        50%  I made my decision based on a comparison of th...  \n",
       "3        70%  I compared the titles of the two products and ...  \n",
       "4        80%  I compared the titles and descriptions of the ...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstructured_analysis(start, end):\n",
    "    llm = ChatOpenAI(temperature=0, model_name=MODEL1)\n",
    "    chain = LLMChain(llm=llm, prompt=UNSTRUCTURED_ANALYSIS_PROMPT, callbacks=[handler])\n",
    "    \n",
    "    for i in range(start, end):\n",
    "        print(i)\n",
    "        whole_prompt = df[\"prompt\"][i]\n",
    "        user_prompt = \"Q:\" + whole_prompt.split(\"Q:\")[-1]\n",
    "        ai_answer = df[\"completion\"][i]\n",
    "        if df[\"predicted_label\"][i]:\n",
    "            prediction = \"Match\"\n",
    "        else:\n",
    "            prediction = \"No Match\"\n",
    "        ai_answer = ai_answer + \"\\n\" + prediction\n",
    "        try:\n",
    "            output = chain.run(user_prompt=user_prompt, ai_answer=ai_answer, callbacks=[handler])\n",
    "        except:\n",
    "            output = \"error\"\n",
    "            i = i - 1\n",
    "            continue\n",
    "\n",
    "        print(output)\n",
    "        df[\"unstructured_analysis\"][i] = output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstructured_analysis(0, len(df))\n",
    "# df.to_csv(DATAPATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIZATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"examples\"],\n",
    "    template=\"\"\"In the following I will give you a few entity matching tasks together with a matching decision, details about the decision and the actual label of the task.\n",
    "Can you please group the wrong decisions into 3-6 fault types and indicate how often each one occurs?\n",
    "{examples}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler2 = CallbackHandler(os.environ.get(\"LF_PK\"), os.environ.get(\"LF_SK\"), os.environ.get(\"LF_HOST\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault types:\n",
      "1. Attribute mismatch: This occurs when the attributes of the two products being compared are different, leading to a wrong decision.\n",
      "2. Category mismatch: This occurs when the two products belong to different categories, but are mistakenly considered as the same.\n",
      "3. Missing information: This occurs when one or both of the products have missing or insufficient information, making it difficult to make an accurate decision.\n",
      "\n",
      "Frequency of fault types:\n",
      "1. Attribute mismatch: 10 occurrences\n",
      "2. Category mismatch: 3 occurrences\n",
      "3. Missing information: 2 occurrences\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=MODEL2)\n",
    "chain = LLMChain(llm=llm, prompt=CATEGORIZATION_PROMPT, callbacks=[handler2])\n",
    "\n",
    "examples = \"\"\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    whole_prompt = df[\"prompt\"][i]\n",
    "    user_prompt = \"Q:\" + whole_prompt.split(\"Q:\")[-1]\n",
    "    examples += \"Task: \" + user_prompt + \"\\n\"\n",
    "    if df[\"predicted_label\"][i]:\n",
    "        prediction = \"Match\"\n",
    "    else:\n",
    "        prediction = \"No Match\"\n",
    "    examples += \"Decision: \" + prediction + \"\\n\"\n",
    "    examples += \"Details: \" + df[\"unstructured_analysis\"][i] + \"\\n\"\n",
    "    examples += \"Label: \" + str(df[\"label\"][i]) + \"\\n\\n\" \n",
    "\n",
    "output = chain.run(examples=examples, callbacks=[handler2])\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
